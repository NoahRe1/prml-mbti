{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2025-01-06T12:34:17.618370Z","iopub.status.busy":"2025-01-06T12:34:17.618045Z","iopub.status.idle":"2025-01-06T12:34:20.202041Z","shell.execute_reply":"2025-01-06T12:34:20.200461Z","shell.execute_reply.started":"2025-01-06T12:34:17.618345Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","Archive:  /usr/share/nltk_data/corpora/wordnet.zip\n","   creating: /usr/share/nltk_data/corpora/wordnet/\n","  inflating: /usr/share/nltk_data/corpora/wordnet/lexnames  \n","  inflating: /usr/share/nltk_data/corpora/wordnet/data.verb  \n","  inflating: /usr/share/nltk_data/corpora/wordnet/index.adv  \n","  inflating: /usr/share/nltk_data/corpora/wordnet/adv.exc  \n","  inflating: /usr/share/nltk_data/corpora/wordnet/index.verb  \n","  inflating: /usr/share/nltk_data/corpora/wordnet/cntlist.rev  \n","  inflating: /usr/share/nltk_data/corpora/wordnet/data.adj  \n","  inflating: /usr/share/nltk_data/corpora/wordnet/index.adj  \n","  inflating: /usr/share/nltk_data/corpora/wordnet/LICENSE  \n","  inflating: /usr/share/nltk_data/corpora/wordnet/citation.bib  \n","  inflating: /usr/share/nltk_data/corpora/wordnet/noun.exc  \n","  inflating: /usr/share/nltk_data/corpora/wordnet/verb.exc  \n","  inflating: /usr/share/nltk_data/corpora/wordnet/README  \n","  inflating: /usr/share/nltk_data/corpora/wordnet/index.sense  \n","  inflating: /usr/share/nltk_data/corpora/wordnet/data.noun  \n","  inflating: /usr/share/nltk_data/corpora/wordnet/data.adv  \n","  inflating: /usr/share/nltk_data/corpora/wordnet/index.noun  \n","  inflating: /usr/share/nltk_data/corpora/wordnet/adj.exc  \n"]}],"source":["import pandas as pd\n","import numpy as np\n","import re\n","import nltk\n","import json\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import TfidfTransformer\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","\n","nltk.download('wordnet')\n","nltk.download('punkt')\n","\n","!unzip /usr/share/nltk_data/corpora/wordnet.zip -d /usr/share/nltk_data/corpora/\n","\n","random_state = 42"]},{"cell_type":"markdown","metadata":{},"source":["# 一、数据预处理"]},{"cell_type":"markdown","metadata":{},"source":["## 1. 数据加载"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2025-01-06T12:34:20.204476Z","iopub.status.busy":"2025-01-06T12:34:20.203727Z","iopub.status.idle":"2025-01-06T12:34:22.401741Z","shell.execute_reply":"2025-01-06T12:34:22.400767Z","shell.execute_reply.started":"2025-01-06T12:34:20.204441Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>type</th>\n","      <th>posts</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>INFJ</td>\n","      <td>'http://www.youtube.com/watch?v=qsXHcwe3krw|||...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>ENTP</td>\n","      <td>'I'm finding the lack of me in these posts ver...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>INTP</td>\n","      <td>'Good one  _____   https://www.youtube.com/wat...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>INTJ</td>\n","      <td>'Dear INTP,   I enjoyed our conversation the o...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>ENTJ</td>\n","      <td>'You're fired.|||That's another silly misconce...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   type                                              posts\n","0  INFJ  'http://www.youtube.com/watch?v=qsXHcwe3krw|||...\n","1  ENTP  'I'm finding the lack of me in these posts ver...\n","2  INTP  'Good one  _____   https://www.youtube.com/wat...\n","3  INTJ  'Dear INTP,   I enjoyed our conversation the o...\n","4  ENTJ  'You're fired.|||That's another silly misconce..."]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["df = pd.read_csv(\"../input/mbti-type/mbti_1.csv\")\n","df.head()"]},{"cell_type":"markdown","metadata":{},"source":["## 2. 数据预处理"]},{"cell_type":"markdown","metadata":{},"source":["将MBTI类型转为序号："]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2025-01-06T12:34:22.404244Z","iopub.status.busy":"2025-01-06T12:34:22.403954Z","iopub.status.idle":"2025-01-06T12:34:22.421286Z","shell.execute_reply":"2025-01-06T12:34:22.420255Z","shell.execute_reply.started":"2025-01-06T12:34:22.404219Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>type</th>\n","      <th>posts</th>\n","      <th>type_idx</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>INFJ</td>\n","      <td>'http://www.youtube.com/watch?v=qsXHcwe3krw|||...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>ENTP</td>\n","      <td>'I'm finding the lack of me in these posts ver...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>INTP</td>\n","      <td>'Good one  _____   https://www.youtube.com/wat...</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>INTJ</td>\n","      <td>'Dear INTP,   I enjoyed our conversation the o...</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>ENTJ</td>\n","      <td>'You're fired.|||That's another silly misconce...</td>\n","      <td>4</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   type                                              posts  type_idx\n","0  INFJ  'http://www.youtube.com/watch?v=qsXHcwe3krw|||...         0\n","1  ENTP  'I'm finding the lack of me in these posts ver...         1\n","2  INTP  'Good one  _____   https://www.youtube.com/wat...         2\n","3  INTJ  'Dear INTP,   I enjoyed our conversation the o...         3\n","4  ENTJ  'You're fired.|||That's another silly misconce...         4"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["unique_type_list = ['INFJ', 'ENTP', 'INTP', 'INTJ', 'ENTJ', 'ENFJ', 'INFP', 'ENFP',\n","                    'ISFP', 'ISTP', 'ISFJ', 'ISTJ', 'ESTP', 'ESFP', 'ESTJ', 'ESFJ']\n","type_to_idx = {mbti_type: idx for idx, mbti_type in enumerate(unique_type_list)}\n","\n","df['type_idx'] = df['type'].map(type_to_idx)\n","df.head()"]},{"cell_type":"markdown","metadata":{},"source":["类别不平衡权重的计算："]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2025-01-06T12:34:22.423257Z","iopub.status.busy":"2025-01-06T12:34:22.422966Z","iopub.status.idle":"2025-01-06T12:34:22.455988Z","shell.execute_reply":"2025-01-06T12:34:22.455025Z","shell.execute_reply.started":"2025-01-06T12:34:22.423234Z"},"trusted":true},"outputs":[{"data":{"text/plain":["[0.36883503401360546,\n"," 0.7915145985401459,\n"," 0.41578796012269936,\n"," 0.4969637946837763,\n"," 2.347132034632035,\n"," 2.8536184210526314,\n"," 0.2959538755458515,\n"," 0.8032407407407407,\n"," 2.000691881918819,\n"," 1.6088649851632046,\n"," 3.2661897590361444,\n"," 2.644817073170732,\n"," 6.091994382022472,\n"," 11.295572916666666,\n"," 13.90224358974359,\n"," 12.90922619047619]"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["type_count = df.groupby(['type']).count()\n","type_count_sum = len(df)\n","\n","class_weights = []\n","\n","for mbti_type in unique_type_list:\n","    class_weight = type_count_sum / (type_count.loc[mbti_type, 'posts'] * len(type_count))\n","    class_weights.append(class_weight)\n","\n","class_weights"]},{"cell_type":"markdown","metadata":{},"source":["读取 YouTube URL 到标题的映射文件："]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2025-01-06T12:34:22.457275Z","iopub.status.busy":"2025-01-06T12:34:22.456989Z","iopub.status.idle":"2025-01-06T12:34:22.739522Z","shell.execute_reply":"2025-01-06T12:34:22.738572Z","shell.execute_reply.started":"2025-01-06T12:34:22.457251Z"},"trusted":true},"outputs":[],"source":["import json\n","\n","youtube_title_file = json.load(open(\"/kaggle/input/mbti-youtube/youtube_video_info.json\"))\n","\n","youtube_title = {}\n","for line in youtube_title_file:\n","    youtube_title[line['url']] = line['title']"]},{"cell_type":"markdown","metadata":{},"source":["# 二、统计特征提取"]},{"cell_type":"markdown","metadata":{},"source":["文本清洗，包括将 YouTube URL 替换为标题："]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2025-01-06T12:34:22.740952Z","iopub.status.busy":"2025-01-06T12:34:22.740552Z","iopub.status.idle":"2025-01-06T12:35:48.754967Z","shell.execute_reply":"2025-01-06T12:35:48.753732Z","shell.execute_reply.started":"2025-01-06T12:34:22.740915Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["posts:  finding lack post alarming sex boring position often example girlfriend currently environment creatively use cowgirl missionary enough giving new meaning game theory hello entp grin take converse flirting acknowledge presence return word smooth wordplay cheeky grin lack balance hand eye coordination real iq test score internet iq test funny score higher like former response thread mention believe iq test banish know entp vanish site year half return find people still commenting post liking idea thought know entp think thing sometimes go old sherlock holmes quote perhaps man special knowledge special power like rather encourages seek complex cheshirewolf tumblr com post really never thought e j p real function judge use use ne ti dominates fe emotion rarely si also use ni due strength know though ingenious saying really want try see happens playing first person shooter back drive around want see look rock paper one best make lol guy lucky really high tumblr system hear new first person shooter game rocking hell soundtrack auto sound equipment shake heaven managed put couple p way connected thing ne ne dominates aware environment se dominates example shawn spencer patrick jane entps well charlie first admit get jealous like chalk w heart mixed dominate w like noticed like known upload clip mic away mouth hear anything ninja assassin style splatter tik tok really great song long mental block singer love beat make bounce drop io v swck mic really close mouth smokin ace assassin ball playing background sociable extrovert extrovert sociable sherlock movie entp normally played extj book estj said movie looked good except called sherlock holmes oh never fear kissing guy kiss animal nothing vanish personal taste liking guy kissed know one sound pretty much like area going right trying figure way want take life want many thing biggest problem know operating impression female never looked boxy okay help gay friend time one developed little crush get red described living worst nightmare trapped one place one one around dull wood serial killer would perfect place sadly tbh biased sound like shadowed infp think maybe hurt turned estj tell typical infp trait left check list sorry seems came bad time already reached quota infjs however female like female make deal kick one antp leaning toward e easy entps intps identify also imagine entp interrogation would go little bit like jack except mechanical rigging shock treatment equipment abandoned building old car batty jumper compliment trust psychopathic except emoticon weird one like laughing get hurt people running lawn mower like theme live know heart usual leave thing end mean time time work thing work mine mbp pleasure meet damn need trust instinct would closer going say infp exfp leaning toward way responded friend even gay lesbian one always come advice bow entp master entps great entps able build building duck duck duck shotgun never hard sad losing someone like knew right give big pat back awesome always correct oh tell stupid know play make laugh going take neuropsychology psychologist nightowl wake pm stay awake till personal opinion backed theory would suggest intps socially difficult intjs socially indifferent also use social situation need arises personal stock desktop downloaded random stock site stock photobuckets tell open photoshop glad like static thanks made friend several hour work constructed every line static get avatar later one fellow teammate psychologist keep around long enough diagnosis like toy diagnosis psychologist friend friend tell \n","type: 1\n"]}],"source":["# 词形还原\n","lemmatiser = WordNetLemmatizer()\n","\n","# 停用词\n","useless_words = stopwords.words(\"english\")\n","\n","# 提取YouTube链接\n","def extract_youtube_links(posts_str):\n","    youtube_pattern = r'https://(?:www\\.)?(?:youtube\\.com/(?:watch\\?v=|embed/)|youtu\\.be/)[\\w\\-]+'\n","    \n","    # 拆分多条发言\n","    posts = posts_str.split('|||')\n","    # 提取所有匹配的YouTube链接\n","    links = []\n","    for post in posts:\n","        # 使用 re.findall 提取完整链接\n","        links.extend(re.findall(youtube_pattern, post))\n","    return links\n","\n","# 数据预处理\n","def pre_process_text(data, remove_stop_words=True, remove_mbti_profiles=True, replace_youtube_url=True):\n","    list_personality = []\n","    list_posts = []\n","    \n","    for row in data.iterrows():\n","        posts = row[1].posts\n","        temp = posts\n","\n","        # 将 YouTube URL 替换为标题\n","        if replace_youtube_url:\n","            youtube_urls = extract_youtube_links(temp)\n","            for youtube_url in youtube_urls:\n","                temp = temp.replace(youtube_url, youtube_title.get(youtube_url, youtube_url))\n","        \n","        # 移除URL链接\n","        temp = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', ' ', temp)\n","        \n","        # 去除非字母字符 - 只保留单词\n","        temp = re.sub(\"[^a-zA-Z]\", \" \", temp)\n","        \n","        # 移除多余的空格（大于1个的空格）\n","        temp = re.sub(' +', ' ', temp).lower()\n","        \n","        # 移除重复字母的单词\n","        temp = re.sub(r'([a-z])\\1{2,}[\\s|\\w]*', '', temp)\n","        \n","        # 移除停用词\n","        if remove_stop_words:\n","            temp = \" \".join([lemmatiser.lemmatize(w) for w in temp.split(' ') if w not in useless_words])\n","        else:\n","            temp = \" \".join([lemmatiser.lemmatize(w) for w in temp.split(' ')])\n","        \n","        # 从帖子中移除MBTI人格类型词汇\n","        if remove_mbti_profiles:\n","            for t in unique_type_list:\n","                temp = temp.replace(t.lower(),\"mbtitype\")\n","\n","        list_posts.append(temp)\n","        list_personality.append(row[1].type_idx)\n","    \n","    # 返回结果\n","    list_posts = np.array(list_posts)\n","    list_personality = np.array(list_personality)\n","    return list_posts, list_personality\n","\n","# 预处理\n","df_copy = df.copy()\n","list_posts, list_personality = pre_process_text(df_copy, remove_stop_words=True, remove_mbti_profiles=False, replace_youtube_url=False)\n","list_posts_youtube, list_personality_youtube = pre_process_text(df_copy, remove_stop_words=True, remove_mbti_profiles=False, replace_youtube_url=True)\n","\n","# 展示一个样本\n","print('posts:', list_posts[1])\n","print('type:', list_personality[1])"]},{"cell_type":"markdown","metadata":{},"source":["将文本转为TF-IDF特征："]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2025-01-06T12:35:48.756452Z","iopub.status.busy":"2025-01-06T12:35:48.756148Z","iopub.status.idle":"2025-01-06T12:35:55.904379Z","shell.execute_reply":"2025-01-06T12:35:55.903274Z","shell.execute_reply.started":"2025-01-06T12:35:48.756427Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["(8675, 615)\n"]}],"source":["# 文本 -> TF\n","cntizer = CountVectorizer(analyzer=\"word\", \n","                             max_features=1000,  \n","                             max_df=0.7,\n","                             min_df=0.1) \n","X_cnt = cntizer.fit_transform(list_posts)\n","X_cnt_youtube = cntizer.fit_transform(list_posts_youtube)\n","\n","# TF -> TF-IDF\n","tfizer = TfidfTransformer()\n","X_tfidf = tfizer.fit_transform(X_cnt).toarray()\n","X_tfidf_youtube = tfizer.fit_transform(X_cnt_youtube).toarray()\n","print(X_tfidf.shape)"]},{"cell_type":"markdown","metadata":{},"source":["# 三、语义特征提取"]},{"cell_type":"markdown","metadata":{},"source":["由于提取时间较长，使用下面的代码将提取出的特征进行保存："]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2025-01-06T12:35:55.907235Z","iopub.status.busy":"2025-01-06T12:35:55.906934Z","iopub.status.idle":"2025-01-06T12:35:55.911904Z","shell.execute_reply":"2025-01-06T12:35:55.910818Z","shell.execute_reply.started":"2025-01-06T12:35:55.907207Z"},"trusted":true},"outputs":[],"source":["# import torch\n","# from tqdm import tqdm\n","# from transformers import pipeline\n","# from transformers import BertTokenizer, BertForSequenceClassification\n","\n","# # 加载预训练的 BERT 模型和分词器\n","# model_name = 'bhadresh-savani/bert-base-uncased-emotion'\n","# tokenizer = BertTokenizer.from_pretrained(model_name)\n","# model = BertForSequenceClassification.from_pretrained(model_name, output_hidden_states=True, return_dict=True).cuda()\n","\n","# # # 获取文本的 BERT 特征\n","# # def get_bert_embeddings(text):\n","# #     with torch.no_grad():\n","# #         inputs = tokenizer(text, truncation=True, max_length=512, return_tensors='pt').to('cuda')\n","# #         outputs = model(**inputs)\n","    \n","# #         # 获取 [CLS] token\n","# #         embeddings = outputs['hidden_states'][-1][:, 0, :].squeeze()\n","\n","# #         # 获取概率\n","# #         logits = outputs['logits']\n","# #         predictions = torch.nn.functional.softmax(logits, dim=-1)\n","    \n","# #         return embeddings, predictions\n","\n","# # 获取文本的情感分类\n","# def get_bert_embeddings(text):\n","#     with torch.no_grad():\n","#         inputs = tokenizer(text, truncation=True, max_length=512, return_tensors='pt').to('cuda')\n","#         outputs = model(**inputs)\n","#         predictions = torch.nn.functional.softmax(outputs['logits'], dim=-1)\n","    \n","#         return predictions\n","\n","# # 数据预处理\n","# def pre_process_text_bert(data, remove_mbti_profiles=True, replace_youtube_url=True):\n","#     list_embeddings = []\n","    \n","#     for row in tqdm(data.iterrows(), total=len(data)):\n","#         posts = row[1].posts\n","#         temp = posts\n","\n","#         # 将 YouTube URL 替换为标题\n","#         if replace_youtube_url:\n","#             youtube_urls = extract_youtube_links(temp)\n","#             for youtube_url in youtube_urls:\n","#                 temp = temp.replace(youtube_url, youtube_title.get(youtube_url, youtube_url))\n","        \n","#         # 移除URL链接\n","#         temp = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', ' ', temp)\n","        \n","#         # 从帖子中移除MBTI人格类型词汇增强模型泛化性\n","#         if remove_mbti_profiles:\n","#             for t in unique_type_list:\n","#                 temp = temp.replace(t.lower(),\"mbtitype\")\n","\n","#         # 提取特征\n","#         emotions = [0] * 6\n","#         with torch.no_grad():\n","#             for post in temp.split('|||'):\n","#                 if any(char.isalpha() for char in post):\n","#                     scores = get_bert_embeddings(post)\n","#                     if scores.max() > 0.9:\n","#                         emotions[scores.argmax().item()] += 1\n","\n","#         emotions = np.array(emotions)\n","#         if emotions.sum().item() == 0:\n","#             emotions = np.zeros((6))\n","#         else:\n","#             emotions = emotions / emotions.sum()\n","#         list_embeddings.append(emotions)\n","    \n","#     # 返回结果\n","#     list_embeddings = np.array(list_embeddings)\n","#     return list_embeddings\n","\n","# # 预处理\n","# df_copy = df.copy()\n","# X_bert = pre_process_text_bert(df_copy, remove_mbti_profiles=False, replace_youtube_url=False)\n","# np.save(\"bert.npy\", X_bert)\n","\n","# # 展示一个样本\n","# print(X_bert[1].shape)"]},{"cell_type":"markdown","metadata":{},"source":["加载保存的特征："]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2025-01-06T12:35:55.913363Z","iopub.status.busy":"2025-01-06T12:35:55.913095Z","iopub.status.idle":"2025-01-06T12:35:55.969655Z","shell.execute_reply":"2025-01-06T12:35:55.968748Z","shell.execute_reply.started":"2025-01-06T12:35:55.913340Z"},"trusted":true},"outputs":[],"source":["X_bert = np.load(\"/kaggle/input/bert-feature/bert.npy\")\n","X_bert = np.concatenate([X_bert, X_tfidf_youtube], axis=1)"]},{"cell_type":"markdown","metadata":{},"source":["# 四、实验"]},{"cell_type":"markdown","metadata":{},"source":["按7:3的比例划分数据集："]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2025-01-06T12:35:55.970742Z","iopub.status.busy":"2025-01-06T12:35:55.970455Z","iopub.status.idle":"2025-01-06T12:35:56.031261Z","shell.execute_reply":"2025-01-06T12:35:56.030196Z","shell.execute_reply.started":"2025-01-06T12:35:55.970718Z"},"trusted":true},"outputs":[],"source":["X_train, X_test, y_train, y_test = train_test_split(X_tfidf, list_personality, test_size=0.3, random_state=random_state)\n","X_train_youtube, X_test_youtube, _, _ = train_test_split(X_tfidf_youtube, list_personality_youtube, test_size=0.3, random_state=random_state)\n","X_train_bert, X_test_bert, _, _ = train_test_split(X_bert, list_personality_youtube, test_size=0.3, random_state=random_state)"]},{"cell_type":"markdown","metadata":{},"source":["定义评估函数："]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2025-01-06T12:35:56.032624Z","iopub.status.busy":"2025-01-06T12:35:56.032327Z","iopub.status.idle":"2025-01-06T12:35:56.039336Z","shell.execute_reply":"2025-01-06T12:35:56.038421Z","shell.execute_reply.started":"2025-01-06T12:35:56.032590Z"},"trusted":true},"outputs":[],"source":["def idx_to_type(idx_array):\n","    IE, NS, TF, JP = [], [], [], []\n","    for idx in idx_array:\n","        mbti = unique_type_list[idx]\n","        IE.append(mbti[0] == 'E')\n","        NS.append(mbti[1] == 'S')\n","        TF.append(mbti[2] == 'F')\n","        JP.append(mbti[3] == 'P')\n","\n","    return IE, NS, TF, JP\n","\n","def evaluate(y_pred):\n","    # 计算总体准确率\n","    total_acc = accuracy_score(y_test, y_pred)\n","\n","    # 拆成四个维度\n","    y_test_IE, y_test_NS, y_test_TF, y_test_JP = idx_to_type(y_test)\n","    y_pred_IE, y_pred_NS, y_pred_TF, y_pred_JP = idx_to_type(y_pred)\n","\n","    # 计算各维度准确率\n","    IE_acc = accuracy_score(y_test_IE, y_pred_IE)\n","    NS_acc = accuracy_score(y_test_NS, y_pred_NS)\n","    TF_acc = accuracy_score(y_test_TF, y_pred_TF)\n","    JP_acc = accuracy_score(y_test_JP, y_pred_JP)\n","    \n","    return {'IE': IE_acc, 'NS':NS_acc, 'TF':TF_acc, 'JP':JP_acc, 'Total': total_acc}"]},{"cell_type":"markdown","metadata":{"execution":{"iopub.execute_input":"2025-01-05T08:10:30.456334Z","iopub.status.busy":"2025-01-05T08:10:30.455964Z","iopub.status.idle":"2025-01-05T08:10:30.460986Z","shell.execute_reply":"2025-01-05T08:10:30.459707Z","shell.execute_reply.started":"2025-01-05T08:10:30.456308Z"}},"source":["## 1. Baseline"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2025-01-06T12:35:56.040594Z","iopub.status.busy":"2025-01-06T12:35:56.040285Z","iopub.status.idle":"2025-01-06T12:36:22.712811Z","shell.execute_reply":"2025-01-06T12:36:22.711919Z","shell.execute_reply.started":"2025-01-06T12:35:56.040569Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["{'IE': 0.8079139454475605, 'NS': 0.88897426046869, 'TF': 0.7921628889742605, 'JP': 0.7598924318094507, 'Total': 0.5485977718017672}\n"]}],"source":["from sklearn.svm import SVC\n","\n","# 在训练数据上拟合模型\n","model = SVC(random_state=random_state)\n","model.fit(X_train, y_train)\n","\n","# 对测试数据进行预测\n","y_pred = model.predict(X_test)\n","print(evaluate(y_pred))"]},{"cell_type":"markdown","metadata":{},"source":["## 2. 加入 YouTube 标题"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2025-01-06T12:36:22.714121Z","iopub.status.busy":"2025-01-06T12:36:22.713769Z","iopub.status.idle":"2025-01-06T12:36:48.984094Z","shell.execute_reply":"2025-01-06T12:36:48.982925Z","shell.execute_reply.started":"2025-01-06T12:36:22.714095Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["{'IE': 0.8086822896657703, 'NS': 0.8885900883595851, 'TF': 0.7917787168651556, 'JP': 0.7645024971187092, 'Total': 0.5509028044563965}\n"]}],"source":["from sklearn.svm import SVC\n","\n","# 在训练数据上拟合模型\n","model = SVC(random_state=random_state)\n","model.fit(X_train_youtube, y_train)\n","\n","# 对测试数据进行预测\n","y_pred = model.predict(X_test_youtube)\n","print(evaluate(y_pred))"]},{"cell_type":"markdown","metadata":{},"source":["## 3. 类别加权"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2025-01-06T12:36:48.985485Z","iopub.status.busy":"2025-01-06T12:36:48.985138Z","iopub.status.idle":"2025-01-06T12:37:17.233499Z","shell.execute_reply":"2025-01-06T12:37:17.232242Z","shell.execute_reply.started":"2025-01-06T12:36:48.985446Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["{'IE': 0.79369957741068, 'NS': 0.8905109489051095, 'TF': 0.8079139454475605, 'JP': 0.7752593161736457, 'Total': 0.5597387629658087}\n"]}],"source":["from sklearn.svm import SVC\n","\n","# 在训练数据上拟合模型\n","model = SVC(random_state=random_state, class_weight={i: weight for i, weight in enumerate(class_weights)})\n","model.fit(X_train, y_train)\n","\n","# 对测试数据进行预测\n","y_pred = model.predict(X_test)\n","print(evaluate(y_pred))"]},{"cell_type":"markdown","metadata":{},"source":["## 4. 加入 YouTube 标题 + 类别加权"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2025-01-06T12:37:17.234748Z","iopub.status.busy":"2025-01-06T12:37:17.234367Z","iopub.status.idle":"2025-01-06T12:37:45.734781Z","shell.execute_reply":"2025-01-06T12:37:45.733817Z","shell.execute_reply.started":"2025-01-06T12:37:17.234710Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["{'IE': 0.7940837495197849, 'NS': 0.8897426046868997, 'TF': 0.8056089127929312, 'JP': 0.774490971955436, 'Total': 0.5605071071840184}\n"]}],"source":["from sklearn.svm import SVC\n","\n","# 在训练数据上拟合模型\n","model = SVC(random_state=random_state, class_weight={i: weight for i, weight in enumerate(class_weights)})\n","model.fit(X_train_youtube, y_train)\n","\n","# 对测试数据进行预测\n","y_pred = model.predict(X_test_youtube)\n","print(evaluate(y_pred))"]},{"cell_type":"markdown","metadata":{},"source":["## 5. 加入 YouTube 标题 + 类别加权 + 语义特征"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2025-01-06T12:37:45.736091Z","iopub.status.busy":"2025-01-06T12:37:45.735758Z","iopub.status.idle":"2025-01-06T12:38:13.834207Z","shell.execute_reply":"2025-01-06T12:38:13.833201Z","shell.execute_reply.started":"2025-01-06T12:37:45.736065Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["{'IE': 0.8025355359200922, 'NS': 0.879369957741068, 'TF': 0.8152132155205533, 'JP': 0.7748751440645409, 'Total': 0.5724164425662697}\n"]}],"source":["from sklearn.svm import SVC\n","\n","# 在训练数据上拟合模型\n","model = SVC(random_state=random_state, class_weight={i: weight for i, weight in enumerate(class_weights)})\n","model.fit(X_train_bert, y_train)\n","\n","# 对测试数据进行预测\n","y_pred = model.predict(X_test_bert)\n","print(evaluate(y_pred))"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"databundleVersionId":4381,"datasetId":2637,"sourceId":4381,"sourceType":"datasetVersion"},{"databundleVersionId":10699472,"datasetId":6431291,"sourceId":10385526,"sourceType":"datasetVersion"},{"databundleVersionId":10695348,"datasetId":6424481,"sourceId":10381845,"sourceType":"datasetVersion"}],"dockerImageVersionId":30822,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
